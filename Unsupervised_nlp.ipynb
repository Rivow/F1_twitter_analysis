{"cells":[{"cell_type":"code","source":"!pip install gensim","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gensim\n  Using cached gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\nCollecting numpy>=1.17.0\n  Using cached numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\nCollecting smart-open>=1.8.1\n  Using cached smart_open-6.0.0-py3-none-any.whl (58 kB)\nCollecting scipy>=0.18.1\n  Using cached scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\nInstalling collected packages: smart-open, numpy, scipy, gensim\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npythonwhat 2.23.1 requires dill~=0.2.7.1, but you have dill 0.3.3 which is incompatible.\npythonwhat 2.23.1 requires jinja2~=2.10, but you have jinja2 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gensim-4.2.0 numpy-1.23.1 scipy-1.9.0 smart-open-6.0.0\n\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/smart_open already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/smart_open-6.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/numpy-1.23.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/scipy-1.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/gensim-4.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/gensim already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m--- Logging error ---\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n    extend(render(renderable, render_options))\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n    for render_output in iter_render:\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n    for line in lines:\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n    for segment in segments:\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n    renderable = rich_cast(renderable)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n    renderable = cast_method()\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n    pip_cmd = get_best_invocation_for_this_pip()\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n    if found_executable and os.path.samefile(\n  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n    s2 = os.stat(f2)\nFileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\nCall stack:\n  File \"/usr/local/bin/pip\", line 8, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n    return command.main(cmd_args)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n    return self._main(args)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n    self.handle_pip_version_check(options)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n    pip_self_version_check(session, options)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n    self._log(WARNING, msg, args, **kwargs)\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n    self.handle(record)\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n    self.callHandlers(record)\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n    hdlr.handle(record)\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n    self.emit(record)\n  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n    self.handleError(record)\nMessage: '[present-rich] %s'\nArguments: (UpgradePrompt(old='22.2.1', new='22.2.2'),)\n","output_type":"stream"}],"id":"ea8b77f5-2ce2-4613-b4d7-f19fd9adb2fc"},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nimport pandas as pd","metadata":{},"execution_count":2,"outputs":[],"id":"c94eed92-1f61-4133-9d2d-43a5445347cc"},{"cell_type":"code","source":"df = pd.read_csv('tweets_translated.csv')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":3,"outputs":[],"id":"69437d90-f7c7-4cb9-bdb5-3927d86fbcbb"},{"cell_type":"code","source":"df.dropna(subset=['tweet'],inplace=True)","metadata":{},"execution_count":4,"outputs":[],"id":"d03f7b1c-ab78-4ba4-bed4-5f7cb2a29bb5"},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":5,"outputs":[],"id":"d2036735-8337-43e8-b25e-3b2b8388e799"},{"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'])","metadata":{},"execution_count":153,"outputs":[],"id":"31eb3b82-e812-46a3-801f-1f0f75286c74"},{"cell_type":"code","source":"df = df.sort_values('date')","metadata":{},"execution_count":156,"outputs":[],"id":"186ddb04-27e0-4d5d-8f72-14b48057b5bb"},{"cell_type":"code","source":"dic_week = {}\ndate = df.date.unique()[4]\nweek = 1\nwhile date in df.date.unique():\n    end_week = date + pd.Timedelta(6, 'd')\n    dic_week[f'week: {week}'] = (date, end_week)\n    week += 1 \n    date = end_week + pd.Timedelta(1, 'd')\ndic_week","metadata":{},"execution_count":171,"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"{'week: 1': (numpy.datetime64('2022-07-18T00:00:00.000000000'),\n  Timestamp('2022-07-24 00:00:00')),\n 'week: 2': (Timestamp('2022-07-25 00:00:00'),\n  Timestamp('2022-07-31 00:00:00')),\n 'week: 3': (Timestamp('2022-08-01 00:00:00'),\n  Timestamp('2022-08-07 00:00:00'))}"},"metadata":{}}],"id":"ee6dd838-4ac8-4cdb-9614-d4103d3feb54"},{"cell_type":"code","source":"for x, y in dic_week.items():\n    t = df[(df.date <= y[1]) & (df.date >= y[0])]\n    print(t.date.unique())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":189,"outputs":[{"name":"stdout","text":"['2022-07-18T00:00:00.000000000' '2022-07-19T00:00:00.000000000'\n '2022-07-20T00:00:00.000000000' '2022-07-23T00:00:00.000000000'\n '2022-07-24T00:00:00.000000000']\n['2022-07-25T00:00:00.000000000' '2022-07-26T00:00:00.000000000'\n '2022-07-27T00:00:00.000000000' '2022-07-28T00:00:00.000000000'\n '2022-07-29T00:00:00.000000000' '2022-07-30T00:00:00.000000000'\n '2022-07-31T00:00:00.000000000']\n['2022-08-01T00:00:00.000000000' '2022-08-02T00:00:00.000000000'\n '2022-08-03T00:00:00.000000000' '2022-08-04T00:00:00.000000000'\n '2022-08-05T00:00:00.000000000']\n","output_type":"stream"}],"id":"81357318-867d-47ec-bf3a-5f57663ab294"},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","metadata":{},"execution_count":6,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"id":"2fdf5712-cd96-4058-b03e-c34a63ce60b7"},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')","metadata":{},"execution_count":7,"outputs":[],"id":"4952a8d8-9335-439d-9d84-91ba459009b1"},{"cell_type":"code","source":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    \n    for token in gensim.utils.simple_preprocess(text):\n        #We accept the word max since it is used for driver Max Verstappen\n        if (token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token != 'live') or token == 'max':\n            \n            result.append(lemmatize_stemming(token))\n            \n    return result","metadata":{},"execution_count":203,"outputs":[],"id":"838243d1-6cd7-442b-a2df-0f4441ff4e2e"},{"cell_type":"code","source":"from gensim import corpora, models\nfrom tqdm import tqdm\nweek_topics = {}\n\nfor week, days in tqdm(dic_week.items()):\n    topics_per_week = {}\n    week_df = df[(df.date <= days[1]) & (df.date >= days[0])]\n    processed = week_df['tweet'].map(preprocess)\n    dictionary = gensim.corpora.Dictionary(processed)\n    \n    # filter words that abear in less than 20% off tweets and keep all popular words\n    dictionary.filter_extremes(no_below=0.2, no_above=0.45, keep_n=100000)\n    \n    # Create a Bag-of-Words model fot every tweet\n    bow_corpus = [dictionary.doc2bow(tweet) for tweet in processed]\n    \n    # creating a TF_IDF model\n    tfidf = models.TfidfModel(bow_corpus)\n    \n    # Apply transformation to the entire corpus call\n    corpus_tfidf = tfidf[bow_corpus]\n    \n    # LDA modeling \n    lda_model = gensim.models.LdaMulticore(corpus_tfidf,\n                                         num_topics=10,\n                                         id2word=dictionary,\n                                         alpha=1,\n                                         passes=2)\n    \n    \n    for idx, topic in lda_model.print_topics(-1):\n        topics_per_week[idx] = topic\n        \n    week_topics[week] = topics_per_week","metadata":{},"execution_count":null,"outputs":[],"id":"1c9ae827-946e-4f03-a7c8-7ecf6764f50a"},{"cell_type":"code","source":"week_topics","metadata":{},"execution_count":202,"outputs":[{"execution_count":202,"output_type":"execute_result","data":{"text/plain":"{'week: 1': {0: '0.012*\"grand\" + 0.012*\"live\" + 0.012*\"prix\" + 0.012*\"stream\" + 0.012*\"watch\" + 0.011*\"frenchgp\" + 0.010*\"𝐋𝐈𝐕𝐄\" + 0.009*\"mercedesamgf\" + 0.008*\"french\" + 0.008*\"franc\"',\n  1: '0.022*\"live\" + 0.015*\"𝐋𝐈𝐕𝐄\" + 0.012*\"stream\" + 0.012*\"prix\" + 0.012*\"watch\" + 0.011*\"frenchgp\" + 0.011*\"grand\" + 0.009*\"lenovo\" + 0.009*\"franc\" + 0.009*\"french\"',\n  2: '0.011*\"live\" + 0.010*\"franc\" + 0.010*\"grand\" + 0.009*\"watch\" + 0.009*\"prix\" + 0.008*\"𝐋𝐈𝐕𝐄\" + 0.008*\"lenovo\" + 0.008*\"frenchgp\" + 0.007*\"stream\" + 0.006*\"lewishamilton\"',\n  3: '0.018*\"live\" + 0.013*\"grand\" + 0.010*\"frenchgp\" + 0.010*\"stream\" + 0.010*\"prix\" + 0.010*\"𝐋𝐈𝐕𝐄\" + 0.010*\"watch\" + 0.008*\"mercedesamgf\" + 0.007*\"franc\" + 0.007*\"french\"',\n  4: '0.014*\"frenchgp\" + 0.014*\"live\" + 0.011*\"stream\" + 0.010*\"prix\" + 0.010*\"grand\" + 0.010*\"𝐋𝐈𝐕𝐄\" + 0.009*\"watch\" + 0.008*\"franc\" + 0.007*\"lenovo\" + 0.007*\"french\"',\n  5: '0.014*\"frenchgp\" + 0.013*\"live\" + 0.011*\"watch\" + 0.010*\"prix\" + 0.010*\"grand\" + 0.009*\"franc\" + 0.009*\"stream\" + 0.009*\"𝐋𝐈𝐕𝐄\" + 0.007*\"leclerc\" + 0.007*\"mercedesamgf\"',\n  6: '0.016*\"live\" + 0.011*\"grand\" + 0.010*\"prix\" + 0.010*\"watch\" + 0.010*\"frenchgp\" + 0.010*\"french\" + 0.009*\"stream\" + 0.008*\"𝐋𝐈𝐕𝐄\" + 0.008*\"mercedesamgf\" + 0.007*\"franc\"',\n  7: '0.016*\"frenchgp\" + 0.014*\"live\" + 0.013*\"𝐋𝐈𝐕𝐄\" + 0.012*\"prix\" + 0.012*\"grand\" + 0.010*\"watch\" + 0.009*\"lenovo\" + 0.009*\"stream\" + 0.008*\"leclerc\" + 0.008*\"franc\"',\n  8: '0.015*\"live\" + 0.013*\"prix\" + 0.011*\"grand\" + 0.010*\"frenchgp\" + 0.008*\"stream\" + 0.008*\"mercedesamgf\" + 0.008*\"watch\" + 0.008*\"lenovo\" + 0.007*\"franc\" + 0.007*\"race\"',\n  9: '0.014*\"frenchgp\" + 0.012*\"stream\" + 0.011*\"𝐋𝐈𝐕𝐄\" + 0.011*\"live\" + 0.011*\"lenovo\" + 0.011*\"prix\" + 0.010*\"watch\" + 0.010*\"grand\" + 0.010*\"franc\" + 0.008*\"french\"'},\n 'week: 2': {0: '0.018*\"hungari\" + 0.018*\"formula\" + 0.016*\"stop\" + 0.009*\"live\" + 0.008*\"verstappen\" + 0.007*\"max\" + 0.007*\"hungariangp\" + 0.006*\"ferrari\" + 0.006*\"prix\" + 0.006*\"grand\"',\n  1: '0.020*\"stop\" + 0.014*\"hungari\" + 0.011*\"formula\" + 0.010*\"live\" + 0.008*\"hungariangp\" + 0.007*\"verstappen\" + 0.007*\"ferrari\" + 0.007*\"max\" + 0.006*\"race\" + 0.006*\"driver\"',\n  2: '0.019*\"hungari\" + 0.014*\"formula\" + 0.014*\"stop\" + 0.010*\"hungariangp\" + 0.009*\"live\" + 0.008*\"max\" + 0.008*\"verstappen\" + 0.005*\"ferrari\" + 0.005*\"race\" + 0.005*\"vettel\"',\n  3: '0.024*\"stop\" + 0.021*\"hungari\" + 0.020*\"formula\" + 0.008*\"max\" + 0.007*\"verstappen\" + 0.006*\"race\" + 0.006*\"live\" + 0.006*\"ferrari\" + 0.006*\"grand\" + 0.005*\"hungariangp\"',\n  4: '0.017*\"hungari\" + 0.016*\"stop\" + 0.013*\"formula\" + 0.011*\"max\" + 0.007*\"live\" + 0.007*\"verstappen\" + 0.007*\"redbullrac\" + 0.006*\"hungariangp\" + 0.006*\"race\" + 0.005*\"team\"',\n  5: '0.017*\"stop\" + 0.015*\"hungari\" + 0.013*\"formula\" + 0.009*\"max\" + 0.008*\"live\" + 0.008*\"verstappen\" + 0.007*\"hungariangp\" + 0.006*\"ferrari\" + 0.006*\"redbullrac\" + 0.006*\"hungarian\"',\n  6: '0.026*\"hungari\" + 0.021*\"formula\" + 0.017*\"stop\" + 0.008*\"verstappen\" + 0.007*\"hungariangp\" + 0.006*\"ferrari\" + 0.006*\"race\" + 0.005*\"max\" + 0.005*\"team\" + 0.005*\"redbullrac\"',\n  7: '0.023*\"stop\" + 0.021*\"hungari\" + 0.018*\"formula\" + 0.010*\"max\" + 0.009*\"verstappen\" + 0.007*\"race\" + 0.006*\"live\" + 0.006*\"hungariangp\" + 0.006*\"vettel\" + 0.005*\"hungarian\"',\n  8: '0.021*\"stop\" + 0.021*\"formula\" + 0.016*\"hungari\" + 0.008*\"hungariangp\" + 0.007*\"max\" + 0.006*\"verstappen\" + 0.006*\"live\" + 0.006*\"race\" + 0.006*\"ferrari\" + 0.005*\"grand\"',\n  9: '0.020*\"formula\" + 0.017*\"stop\" + 0.016*\"hungari\" + 0.010*\"live\" + 0.009*\"hungariangp\" + 0.008*\"verstappen\" + 0.007*\"max\" + 0.007*\"race\" + 0.006*\"redbullrac\" + 0.006*\"ferrari\"'},\n 'week: 3': {0: '0.014*\"alex_albon\" + 0.011*\"formula\" + 0.010*\"astonmartinf\" + 0.009*\"williamsrac\" + 0.008*\"team\" + 0.007*\"alpinef\" + 0.007*\"oscarpiastri\" + 0.005*\"mercedesamgf\" + 0.005*\"driver\" + 0.005*\"alpin\"',\n  1: '0.013*\"formula\" + 0.011*\"astonmartinf\" + 0.011*\"williamsrac\" + 0.008*\"alpinef\" + 0.008*\"oscarpiastri\" + 0.008*\"mercedesamgf\" + 0.007*\"team\" + 0.007*\"alex_albon\" + 0.005*\"alonso\" + 0.005*\"alpin\"',\n  2: '0.011*\"formula\" + 0.011*\"team\" + 0.011*\"alpinef\" + 0.010*\"astonmartinf\" + 0.009*\"oscarpiastri\" + 0.008*\"alex_albon\" + 0.008*\"williamsrac\" + 0.006*\"mercedesamgf\" + 0.006*\"alonso\" + 0.005*\"alo_ofici\"',\n  3: '0.011*\"alpinef\" + 0.011*\"williamsrac\" + 0.011*\"formula\" + 0.011*\"alex_albon\" + 0.010*\"oscarpiastri\" + 0.009*\"team\" + 0.008*\"astonmartinf\" + 0.007*\"mercedesamgf\" + 0.005*\"alpin\" + 0.004*\"driver\"',\n  4: '0.012*\"formula\" + 0.011*\"team\" + 0.010*\"astonmartinf\" + 0.010*\"williamsrac\" + 0.009*\"alex_albon\" + 0.009*\"alpinef\" + 0.008*\"mercedesamgf\" + 0.007*\"oscarpiastri\" + 0.006*\"alonso\" + 0.006*\"alpin\"',\n  5: '0.015*\"astonmartinf\" + 0.012*\"formula\" + 0.010*\"oscarpiastri\" + 0.008*\"team\" + 0.008*\"alpinef\" + 0.008*\"alex_albon\" + 0.008*\"williamsrac\" + 0.007*\"mercedesamgf\" + 0.007*\"alpin\" + 0.006*\"driver\"',\n  6: '0.013*\"team\" + 0.011*\"astonmartinf\" + 0.011*\"formula\" + 0.010*\"williamsrac\" + 0.008*\"alex_albon\" + 0.008*\"oscarpiastri\" + 0.008*\"mercedesamgf\" + 0.008*\"alpinef\" + 0.005*\"piastri\" + 0.005*\"alpin\"',\n  7: '0.013*\"formula\" + 0.013*\"team\" + 0.010*\"alex_albon\" + 0.010*\"oscarpiastri\" + 0.010*\"williamsrac\" + 0.010*\"astonmartinf\" + 0.008*\"alpinef\" + 0.006*\"mercedesamgf\" + 0.005*\"race\" + 0.005*\"alonso\"',\n  8: '0.011*\"mercedesamgf\" + 0.011*\"williamsrac\" + 0.011*\"formula\" + 0.010*\"astonmartinf\" + 0.008*\"alpinef\" + 0.008*\"team\" + 0.008*\"oscarpiastri\" + 0.006*\"alex_albon\" + 0.005*\"piastri\" + 0.005*\"alpin\"',\n  9: '0.010*\"astonmartinf\" + 0.010*\"oscarpiastri\" + 0.010*\"formula\" + 0.009*\"team\" + 0.009*\"williamsrac\" + 0.007*\"alpinef\" + 0.006*\"mercedesamgf\" + 0.005*\"alex_albon\" + 0.005*\"alpin\" + 0.005*\"piastri\"'}}"},"metadata":{}}],"id":"6b65467a-b61e-4b09-8835-652cfe5422ea"},{"cell_type":"code","source":"df_date = df[df.date == '2022-07-28']\nprocessed = df_date['tweet'].map(preprocess)","metadata":{},"execution_count":133,"outputs":[],"id":"c8985119-33c4-43d0-8532-56e84cfa7724"},{"cell_type":"code","source":"processed = df['tweet'].map(preprocess)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":97,"outputs":[],"id":"a5320689-1962-4599-8e35-8723b9c81cd3"},{"cell_type":"code","source":"processed[:10]","metadata":{},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"157970                       [emilkolega, hamilton, russel]\n157971    [scene, indianapoli, world, famous, themelodyi...\n157972                 [enthusto, like, skid, mark, underp]\n157973                            [benzytwt, master, benzi]\n157974    [zero_formula, say, innittwt, sure, major, smi...\n157975    [miss, thorough, unafraid, platform, leav, spo...\n157976                     [vega, marshal, vitalvega, sign]\n157977    [indycaronnbc, cgrteam, alexpalou, zack, brown...\n157978    [of_snitch, peterarrindel, gdb_lpn, vega, hell...\n157979                                       [race, denial]\nName: tweet, dtype: object"},"metadata":{}}],"id":"03075f8c-71b4-487c-8fb2-fccc0e239e73"},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(processed)","metadata":{},"execution_count":135,"outputs":[],"id":"2389bba7-5bd9-48b4-b74a-3902b6c7a72f"},{"cell_type":"code","source":"# filter words that abear in less than 20% off tweets and keep all popular words\ndictionary.filter_extremes(no_below=0.2, no_above=0.5, keep_n=100000)","metadata":{},"execution_count":136,"outputs":[],"id":"763e0e89-1cac-40b1-96ef-651747060748"},{"cell_type":"code","source":"# Create a Bag-of-Words model fot every tweet\nbow_corpus = [dictionary.doc2bow(tweet) for tweet in processed]","metadata":{},"execution_count":137,"outputs":[],"id":"f55692d0-75b3-434f-a59b-5f56c4d1b7c0"},{"cell_type":"code","source":"# creating a TF_IDF model\nfrom gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\nprint(tfidf)","metadata":{},"execution_count":138,"outputs":[{"name":"stdout","text":"TfidfModel<num_docs=21789, num_nnz=142798>\n","output_type":"stream"}],"id":"76242368-28c3-44bf-a6ef-693ca97bcb48"},{"cell_type":"code","source":"# Apply transformation to the entire corpus call\ncorpus_tfidf = tfidf[bow_corpus]\nprint(corpus_tfidf[0])","metadata":{},"execution_count":139,"outputs":[{"name":"stdout","text":"[(0, 0.6883612652149576), (1, 0.40181928382705734), (2, 0.6039039921182618)]\n","output_type":"stream"}],"id":"c6146171-d3fa-4e1a-8b97-6f5bd14dcf40"},{"cell_type":"code","source":"from pprint import pprint\n\nfor tweet in corpus_tfidf:\n    pprint(tweet)\n    break","metadata":{},"execution_count":140,"outputs":[{"name":"stdout","text":"[(0, 0.6883612652149576), (1, 0.40181928382705734), (2, 0.6039039921182618)]\n","output_type":"stream"}],"id":"700c74b3-739f-44dd-a3f2-1daa574c9234"},{"cell_type":"code","source":"# LDA modeling \nlda_model = gensim.models.LdaMulticore(bow_corpus,\n                                     num_topics=10,\n                                     id2word=dictionary,\n                                     alpha=0.75,\n                                     eta=0.5,\n                                     passes=2)","metadata":{},"execution_count":196,"outputs":[{"name":"stderr","text":"  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 721, in inference\n    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 109, in worker\n    initializer(*initargs)\n  File \"<__array_function__ internals>\", line 180, in dot\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamulticore.py\", line 344, in worker_e_step\n    worker_lda.sync_state()\nKeyboardInterrupt\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 721, in inference\n    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 109, in worker\n    initializer(*initargs)\n  File \"<__array_function__ internals>\", line 180, in dot\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamulticore.py\", line 344, in worker_e_step\n    worker_lda.sync_state()\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 721, in inference\n    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 109, in worker\n    initializer(*initargs)\n  File \"<__array_function__ internals>\", line 180, in dot\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 721, in inference\n    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 109, in worker\n    initializer(*initargs)\n  File \"/home/repl/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\", line 721, in inference\n    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n","output_type":"stream"}],"id":"72b3d655-84dd-43d2-a18d-eac18d8e5253"},{"cell_type":"code","source":"for idx, topic in lda_model.print_topics(-1):\n    print(f'Topics {idx} \\nWords: {topic}')\n    print('\\n')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"execution_count":197,"outputs":[{"name":"stdout","text":"Topics 0 \nWords: 0.030*\"formula\" + 0.018*\"team\" + 0.014*\"astonmartinf\" + 0.013*\"alpinef\" + 0.011*\"oscarpiastri\" + 0.008*\"williamsrac\" + 0.008*\"alex_albon\" + 0.007*\"alonso\" + 0.007*\"driver\" + 0.006*\"race\"\n\n\nTopics 1 \nWords: 0.036*\"formula\" + 0.017*\"team\" + 0.011*\"astonmartinf\" + 0.010*\"oscarpiastri\" + 0.009*\"williamsrac\" + 0.009*\"alpinef\" + 0.008*\"alpin\" + 0.008*\"alonso\" + 0.007*\"alex_albon\" + 0.007*\"race\"\n\n\nTopics 2 \nWords: 0.031*\"formula\" + 0.017*\"team\" + 0.010*\"astonmartinf\" + 0.009*\"oscarpiastri\" + 0.008*\"alpinef\" + 0.007*\"alonso\" + 0.006*\"alpin\" + 0.006*\"max\" + 0.006*\"driver\" + 0.006*\"year\"\n\n\nTopics 3 \nWords: 0.041*\"formula\" + 0.012*\"team\" + 0.010*\"astonmartinf\" + 0.010*\"williamsrac\" + 0.009*\"race\" + 0.008*\"alex_albon\" + 0.008*\"alpin\" + 0.007*\"max\" + 0.007*\"alpinef\" + 0.007*\"alonso\"\n\n\nTopics 4 \nWords: 0.031*\"formula\" + 0.014*\"team\" + 0.009*\"race\" + 0.009*\"astonmartinf\" + 0.009*\"max\" + 0.008*\"alpinef\" + 0.008*\"driver\" + 0.008*\"verstappen\" + 0.007*\"alonso\" + 0.006*\"ferrari\"\n\n\nTopics 5 \nWords: 0.024*\"formula\" + 0.018*\"team\" + 0.011*\"astonmartinf\" + 0.010*\"alpinef\" + 0.010*\"oscarpiastri\" + 0.009*\"williamsrac\" + 0.008*\"driver\" + 0.007*\"alex_albon\" + 0.007*\"like\" + 0.006*\"year\"\n\n\nTopics 6 \nWords: 0.033*\"formula\" + 0.022*\"team\" + 0.011*\"alpinef\" + 0.009*\"astonmartinf\" + 0.008*\"williamsrac\" + 0.008*\"oscarpiastri\" + 0.008*\"race\" + 0.007*\"alpin\" + 0.007*\"driver\" + 0.006*\"alex_albon\"\n\n\nTopics 7 \nWords: 0.035*\"formula\" + 0.014*\"team\" + 0.012*\"astonmartinf\" + 0.009*\"williamsrac\" + 0.009*\"oscarpiastri\" + 0.009*\"alpin\" + 0.008*\"alonso\" + 0.008*\"alpinef\" + 0.007*\"alex_albon\" + 0.007*\"race\"\n\n\nTopics 8 \nWords: 0.048*\"formula\" + 0.014*\"team\" + 0.010*\"astonmartinf\" + 0.008*\"max\" + 0.008*\"driver\" + 0.008*\"oscarpiastri\" + 0.007*\"alpinef\" + 0.007*\"alpin\" + 0.007*\"race\" + 0.007*\"year\"\n\n\nTopics 9 \nWords: 0.033*\"formula\" + 0.013*\"team\" + 0.011*\"astonmartinf\" + 0.010*\"race\" + 0.008*\"williamsrac\" + 0.008*\"oscarpiastri\" + 0.007*\"alpinef\" + 0.007*\"driver\" + 0.007*\"alpin\" + 0.007*\"alex_albon\"\n\n\n","output_type":"stream"}],"id":"41a3d5f9-b8ff-4190-8c57-5deaac0ca70d"},{"cell_type":"code","source":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                     num_topics=10,\n                                     id2word=dictionary,\n                                     passes=2)","metadata":{},"execution_count":144,"outputs":[],"id":"333cc4ea-5ad8-4fd7-81a7-8b0af6891811"},{"cell_type":"code","source":"for idx, topic in lda_model_tfidf.print_topics(-1):\n    print(f'Topics {idx} \\nWords: {topic}')\n    print('\\n')","metadata":{},"execution_count":145,"outputs":[{"name":"stdout","text":"Topics 0 \nWords: 0.011*\"thankyouseb\" + 0.011*\"vettel\" + 0.011*\"formula\" + 0.010*\"retir\" + 0.009*\"sebastian\" + 0.008*\"driver\" + 0.008*\"season\" + 0.007*\"team\" + 0.005*\"time\" + 0.005*\"miss\"\n\n\nTopics 1 \nWords: 0.023*\"scuderiaferrari\" + 0.021*\"thank\" + 0.015*\"formula\" + 0.009*\"vettel\" + 0.008*\"miss\" + 0.008*\"great\" + 0.007*\"time\" + 0.006*\"thankyouseb\" + 0.006*\"retir\" + 0.006*\"driver\"\n\n\nTopics 2 \nWords: 0.022*\"thank\" + 0.019*\"miss\" + 0.015*\"sebastian\" + 0.015*\"vettel\" + 0.014*\"formula\" + 0.011*\"retir\" + 0.010*\"legend\" + 0.007*\"announc\" + 0.007*\"season\" + 0.007*\"world\"\n\n\nTopics 3 \nWords: 0.010*\"time\" + 0.009*\"best\" + 0.009*\"formula\" + 0.008*\"vettel\" + 0.007*\"sebastian\" + 0.007*\"retir\" + 0.006*\"driver\" + 0.006*\"season\" + 0.006*\"like\" + 0.005*\"great\"\n\n\nTopics 4 \nWords: 0.011*\"retir\" + 0.011*\"vettel\" + 0.011*\"formula\" + 0.009*\"announc\" + 0.007*\"sebastian\" + 0.007*\"nooooooo\" + 0.007*\"season\" + 0.005*\"career\" + 0.005*\"team\" + 0.005*\"sebastián\"\n\n\nTopics 5 \nWords: 0.011*\"formula\" + 0.009*\"retir\" + 0.009*\"vettel\" + 0.008*\"announc\" + 0.008*\"nooooooooo\" + 0.008*\"cri\" + 0.007*\"sebastian\" + 0.006*\"miss\" + 0.006*\"time\" + 0.006*\"legend\"\n\n\nTopics 6 \nWords: 0.016*\"vettel\" + 0.013*\"formula\" + 0.012*\"retir\" + 0.010*\"sebastian\" + 0.008*\"season\" + 0.007*\"announc\" + 0.005*\"driver\" + 0.005*\"come\" + 0.005*\"year\" + 0.005*\"like\"\n\n\nTopics 7 \nWords: 0.020*\"legend\" + 0.010*\"formula\" + 0.009*\"vettel\" + 0.009*\"retir\" + 0.007*\"driver\" + 0.006*\"blog_formula\" + 0.006*\"race\" + 0.006*\"season\" + 0.006*\"sebastian\" + 0.006*\"sebastianvettel\"\n\n\nTopics 8 \nWords: 0.037*\"vettel\" + 0.036*\"retir\" + 0.035*\"sebastian\" + 0.033*\"formula\" + 0.028*\"announc\" + 0.026*\"season\" + 0.014*\"champion\" + 0.012*\"time\" + 0.011*\"sebastianvettel\" + 0.010*\"world\"\n\n\nTopics 9 \nWords: 0.009*\"legend\" + 0.007*\"formula\" + 0.007*\"vorestappen\" + 0.006*\"race\" + 0.006*\"nooooo\" + 0.006*\"win\" + 0.006*\"hamilton\" + 0.006*\"thank\" + 0.006*\"vettel\" + 0.005*\"good\"\n\n\n","output_type":"stream"}],"id":"0ff4d8a8-0b0b-4539-a46b-6c918d6825b4"}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}