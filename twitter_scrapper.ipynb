{"cells":[{"source":"!pip3 install --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\nimport pandas as pd\nimport twint\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"061c3b95-b61f-4d84-adb1-0b9bef6492db","cell_type":"code","execution_count":3,"outputs":[{"name":"stdout","text":"Collecting twint\n  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-w1kc0opt/twint_a9b0d75fc8dd4e9495f5ad250ac0ed77\n  Running command git clone --filter=blob:none --quiet https://github.com/twintproject/twint.git /tmp/pip-install-w1kc0opt/twint_a9b0d75fc8dd4e9495f5ad250ac0ed77\n\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\u001b[33m\n\u001b[0m  Running command git checkout -q origin/master\n  Resolved https://github.com/twintproject/twint.git to commit origin/master\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting aiohttp\n  Using cached aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\nCollecting aiodns\n  Using cached aiodns-3.0.0-py3-none-any.whl (5.0 kB)\nCollecting beautifulsoup4\n  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\nCollecting cchardet\n  Using cached cchardet-2.1.7-cp38-cp38-manylinux2010_x86_64.whl (265 kB)\nCollecting dataclasses\n  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\nCollecting elasticsearch\n  Using cached elasticsearch-8.3.3-py3-none-any.whl (382 kB)\nCollecting pysocks\n  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\nCollecting pandas\n  Using cached pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\nCollecting aiohttp_socks\n  Using cached aiohttp_socks-0.7.1-py3-none-any.whl (9.3 kB)\nCollecting schedule\n  Using cached schedule-1.1.0-py2.py3-none-any.whl (10 kB)\nCollecting geopy\n  Using cached geopy-2.2.0-py3-none-any.whl (118 kB)\nCollecting fake-useragent\n  Using cached fake_useragent-0.1.11-py3-none-any.whl\nCollecting googletransx\n  Using cached googletransx-2.4.2-py3-none-any.whl\nCollecting pycares>=4.0.0\n  Using cached pycares-4.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (289 kB)\nCollecting yarl<2.0,>=1.0\n  Using cached yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\nCollecting frozenlist>=1.1.1\n  Using cached frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\nCollecting async-timeout<5.0,>=4.0.0a3\n  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting aiosignal>=1.1.2\n  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\nCollecting multidict<7.0,>=4.5\n  Using cached multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\nCollecting attrs>=17.3.0\n  Using cached attrs-22.1.0-py2.py3-none-any.whl (58 kB)\nCollecting charset-normalizer<3.0,>=2.0\n  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\nCollecting python-socks[asyncio]<3.0.0,>=2.0.0\n  Using cached python_socks-2.0.3-py3-none-any.whl (49 kB)\nCollecting soupsieve>1.2\n  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\nCollecting elastic-transport<9,>=8\n  Using cached elastic_transport-8.1.2-py3-none-any.whl (59 kB)\nCollecting geographiclib<2,>=1.49\n  Using cached geographiclib-1.52-py3-none-any.whl (38 kB)\nCollecting requests\n  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\nCollecting python-dateutil>=2.8.1\n  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\nCollecting numpy>=1.18.5\n  Using cached numpy-1.23.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\nCollecting pytz>=2020.1\n  Using cached pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\nCollecting urllib3<2,>=1.26.2\n  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\nCollecting certifi\n  Using cached certifi-2022.6.15-py3-none-any.whl (160 kB)\nCollecting cffi>=1.5.0\n  Using cached cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\nCollecting six>=1.5\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting idna>=2.0\n  Using cached idna-3.3-py3-none-any.whl (61 kB)\nCollecting pycparser\n  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\nBuilding wheels for collected packages: twint\n  Building wheel for twint (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38871 sha256=655867ebb5d2a8a51067f7a934acde29e4e3426e004403f15b924a54be25471e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1vmogwyl/wheels/82/67/14/5495652787d3c55288164de4234c0712a1f785a64f61f1ea70\nSuccessfully built twint\nInstalling collected packages: pytz, python-socks, geographiclib, fake-useragent, dataclasses, cchardet, urllib3, soupsieve, six, schedule, pysocks, pycparser, numpy, multidict, idna, geopy, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, python-dateutil, elastic-transport, cffi, beautifulsoup4, aiosignal, pycares, pandas, googletransx, elasticsearch, aiohttp, aiohttp_socks, aiodns, twint\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nredshift-connector 2.0.900 requires pytz<2021.9,>=2020.1, but you have pytz 2022.2.1 which is incompatible.\nredshift-connector 2.0.900 requires requests<2.26.1,>=2.23.0, but you have requests 2.28.1 which is incompatible.\nmarkdown-it-py 1.1.0 requires attrs<22,>=19, but you have attrs 22.1.0 which is incompatible.\npythonwhat 2.23.1 requires dill~=0.2.7.1, but you have dill 0.3.3 which is incompatible.\npythonwhat 2.23.1 requires jinja2~=2.10, but you have jinja2 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiodns-3.0.0 aiohttp-3.8.1 aiohttp_socks-0.7.1 aiosignal-1.2.0 async-timeout-4.0.2 attrs-22.1.0 beautifulsoup4-4.11.1 cchardet-2.1.7 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.1.1 dataclasses-0.6 elastic-transport-8.1.2 elasticsearch-8.3.3 fake-useragent-0.1.11 frozenlist-1.3.1 geographiclib-1.52 geopy-2.2.0 googletransx-2.4.2 idna-3.3 multidict-6.0.2 numpy-1.23.2 pandas-1.4.3 pycares-4.2.2 pycparser-2.21 pysocks-1.7.1 python-dateutil-2.8.2 python-socks-2.0.3 pytz-2022.2.1 requests-2.28.1 schedule-1.1.0 six-1.16.0 soupsieve-2.3.2.post1 twint-2.1.21 urllib3-1.26.12 yarl-1.8.1\n","output_type":"stream"}]},{"source":"!pip install nest_asyncio\n!pip install tqdm","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"cell_type":"code","id":"a1ec2586-be3c-4289-89ba-24c8a33f7d35","execution_count":5,"outputs":[{"name":"stdout","text":"Collecting nest_asyncio\n  Using cached nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\nInstalling collected packages: nest_asyncio\nSuccessfully installed nest_asyncio-1.5.5\n\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/nest_asyncio.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/nest_asyncio-1.5.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0mCollecting tqdm\n  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\nInstalling collected packages: tqdm\nSuccessfully installed tqdm-4.64.0\n\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/tqdm-4.64.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"source":"try:#check if the file exist so you dont have to scrap from begining\n    df = pd.read_csv('tweets.csv') \n    df = df.drop(df.columns[0], axis=1)\n    df = df.dropna(subset='date')\n    for date in df.date.unique():\n        if len(date.split('-')) != 3:\n            indexes = df[df.date==date].index\n            for index in indexes:\n                df = df.drop(index)\n    df['date'] = pd.to_datetime(df['date'])\n    date = max(df['date'])\n    date = str(date).split(' ')[0] #get the date in the string format for twint\nexcept:\n    print('No tweets file')\ndf.shape","metadata":{},"cell_type":"code","id":"8dab6064-0ba9-48d5-9eee-312af093f674","execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"(611702, 37)"},"metadata":{}}]},{"source":"date","metadata":{},"cell_type":"code","id":"fb518880-fa9d-4f40-922e-22c8baa75319","execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'2022-08-22'"},"metadata":{}}]},{"source":"import nest_asyncio\nnest_asyncio.apply()","metadata":{},"cell_type":"code","id":"ce615dd9-3885-440f-8287-c62004cd2064","execution_count":45,"outputs":[]},{"source":"c = twint.Config()\nc.Search = ['Formula 1']\nc.Limit = 200000\nc.Since = date\nc.Store_csv = True\nc.Output = 'new_tweets.csv'\nc.Hide_output = True\ntwint.run.Search(c)","metadata":{},"cell_type":"code","id":"a30d1af0-bb3e-4a76-b78c-a714cf53adb3","execution_count":46,"outputs":[{"name":"stdout","text":"[!] No more data! Scraping will stop now.\nfound 0 deleted tweets in this search.\n","output_type":"stream"}]},{"source":"df1 = pd.read_csv('new_tweets.csv')\ndf1['date'] = pd.to_datetime(df1['date'])\ndate = max(df1['date'])\ndate = str(date).split(' ')[0] \ndf1['tweet']\ndate","metadata":{},"cell_type":"code","id":"3af7d50b-ac02-470f-8de0-acec7d2f4810","execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'2022-08-23'"},"metadata":{}}]},{"source":"tweets = pd.concat([df, df1]) #mergin both df so you have only one\ntweets = tweets.drop_duplicates()\ntweets.to_csv('tweets.csv', index=False)\ntweets.shape","metadata":{},"cell_type":"code","id":"aacc71a3-ec8d-47ad-b995-fa1261faa620","execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(769404, 37)"},"metadata":{}}]},{"source":"**Filter the new tweets from the old ones to not double translate**","metadata":{},"cell_type":"markdown","id":"ca748972-068f-479b-a346-9019de5e0ab7"},{"source":"df_t = pd.read_csv('tweets_translated.csv')\ndf_t['time_stamp'] = df_t['date'] + ' ' + df_t['time']\ndf_t['time_stamp'] = pd.to_datetime(df_t['time_stamp'])\ndate = max(df_t['time_stamp'])\ndate\ndf_t.shape","metadata":{},"cell_type":"code","id":"2d0eb522-ac19-4f59-8c08-18ab23b0e39b","execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(368842, 41)"},"metadata":{}}]},{"source":"df1 = pd.read_csv('new_tweets.csv')\ndf1['time_stamp'] = df1['date'] + ' ' + df1['time']\ndf1['time_stamp'] = pd.to_datetime(df1['time_stamp'])\nnew = df1[df1['time_stamp'] > date]\nnew.shape","metadata":{},"cell_type":"code","id":"72e0590d-5139-4219-8f3b-65df13ae337c","execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(10776, 37)"},"metadata":{}}]},{"source":"!pip install deep_translator","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"cell_type":"code","id":"3e0601af-c9d7-4c9f-a21f-1dca23dd3080","execution_count":8,"outputs":[{"name":"stdout","text":"Collecting deep_translator\n  Downloading deep_translator-1.8.3-py3-none-any.whl (29 kB)\nCollecting beautifulsoup4<5.0.0,>=4.9.1\n  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\nCollecting requests<3.0.0,>=2.23.0\n  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\nCollecting soupsieve>1.2\n  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\nCollecting urllib3<1.27,>=1.21.1\n  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\nCollecting certifi>=2017.4.17\n  Using cached certifi-2022.6.15-py3-none-any.whl (160 kB)\nCollecting charset-normalizer<3,>=2\n  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\nCollecting idna<4,>=2.5\n  Using cached idna-3.3-py3-none-any.whl (61 kB)\nInstalling collected packages: urllib3, soupsieve, idna, charset-normalizer, certifi, requests, beautifulsoup4, deep_translator\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nredshift-connector 2.0.900 requires pytz<2021.9,>=2020.1, but you have pytz 2022.2.1 which is incompatible.\nredshift-connector 2.0.900 requires requests<2.26.1,>=2.23.0, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed beautifulsoup4-4.11.1 certifi-2022.6.15 charset-normalizer-2.1.1 deep_translator-1.8.3 idna-3.3 requests-2.28.1 soupsieve-2.3.2.post1 urllib3-1.26.12\n\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/urllib3-1.26.12.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/soupsieve-2.3.2.post1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/idna-3.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/charset_normalizer-2.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/certifi-2022.6.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/requests-2.28.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/beautifulsoup4-4.11.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /home/repl/.local/lib/python3.8/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"source":"# Translate tweets for better analysis\nfrom tqdm import tqdm\nfrom deep_translator import GoogleTranslator\ntqdm.pandas()\nnew['tweet'] = new['tweet'].progress_apply(lambda tweet: GoogleTranslator(source='auto', target='en').translate(tweet))\nnew['tweet']","metadata":{},"cell_type":"code","id":"c7ef42a4-66f7-49fc-9d88-7c1d2b5beb3b","execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 10776/10776 [49:13<00:00,  3.65it/s] \n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"576534                 @MrLH44 @F1  https://t.co/gPtJACIYBA\n576535    @vettelfan_niki @F1 @SchumacherMick 🥲  https:/...\n576536    F1 quiz: Every driver to score a point in the ...\n576537    Former Formula 1 chief Ecclestone denies fraud...\n576538     @Lokihan3 @KazuneKZ_ @F1 https://t.co/jfBglZXlUn\n                                ...                        \n587305               @F1Manager @F1 Grab this ratio instead\n587306    # F1 | @Anto_Giovinazzi with #Haas in FP1 at M...\n587307                          @F1Manager @F1 #f1Manager 🔋\n587308                         @F1Manager @F1 #F1Manager Ⓜ️\n587309                     @F1 😁😁😁😁 https://t.co/6wHG24FObs\nName: tweet, Length: 10776, dtype: object"},"metadata":{}}]},{"source":"tweets = pd.concat([new, df_t]) #mergin both df so you have only one\ntweets = tweets.drop_duplicates()\ntweets['tweet']","metadata":{},"cell_type":"code","id":"64e3bbfc-5b81-4a77-8aa0-ee5e4463d87e","execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"576534                 @MrLH44 @F1  https://t.co/gPtJACIYBA\n576535    @vettelfan_niki @F1 @SchumacherMick 🥲  https:/...\n576536    F1 quiz: Every driver to score a point in the ...\n576537    Former Formula 1 chief Ecclestone denies fraud...\n576538     @Lokihan3 @KazuneKZ_ @F1 https://t.co/jfBglZXlUn\n                                ...                        \n368837    1968: Born Ricardo Rosset, Brazilian Formula 1...\n368838    Shitty no one knows how to race with formula 1...\n368839    ((The Most Read)) #Noticias #mundotuerca #form...\n368840    💖🐶 'Fabbio' brings all the attitude of formula...\n368841    @marca @Carlossainz55 RIDICULOUS .... that's h...\nName: tweet, Length: 379618, dtype: object"},"metadata":{}}]},{"source":"tweets = pd.concat([new, df_t]) #mergin both df so you have only one\ntweets = tweets.drop_duplicates()\ntweets.to_csv('tweets_translated.csv', index=False)","metadata":{},"cell_type":"code","id":"f55b6d5f-eafa-4ffa-b19a-97e8b65c097e","execution_count":11,"outputs":[]},{"source":"data = pd.read_csv('tweets_translated.csv')\ndata.info()","metadata":{},"cell_type":"code","id":"2d4c4d28-ae0b-4f2e-9636-9031ace37a21","execution_count":12,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 379618 entries, 0 to 379617\nData columns (total 41 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   id               379618 non-null  object \n 1   conversation_id  307741 non-null  float64\n 2   created_at       307741 non-null  object \n 3   date             379616 non-null  object \n 4   time             379616 non-null  object \n 5   timezone         307741 non-null  float64\n 6   user_id          379616 non-null  float64\n 7   username         379616 non-null  object \n 8   name             379610 non-null  object \n 9   place            91 non-null      object \n 10  tweet            379614 non-null  object \n 11  language         368240 non-null  object \n 12  mentions         368240 non-null  object \n 13  urls             368240 non-null  object \n 14  photos           368240 non-null  object \n 15  replies_count    368240 non-null  float64\n 16  retweets_count   368240 non-null  float64\n 17  likes_count      368240 non-null  float64\n 18  hashtags         368240 non-null  object \n 19  cashtags         368240 non-null  object \n 20  link             368240 non-null  object \n 21  retweet          368240 non-null  object \n 22  quote_url        10350 non-null   object \n 23  video            368240 non-null  float64\n 24  thumbnail        62274 non-null   object \n 25  near             0 non-null       float64\n 26  geo              0 non-null       float64\n 27  source           0 non-null       float64\n 28  user_rt_id       0 non-null       float64\n 29  user_rt          0 non-null       float64\n 30  retweet_id       0 non-null       float64\n 31  reply_to         368240 non-null  object \n 32  retweet_date     0 non-null       float64\n 33  translate        0 non-null       float64\n 34  trans_src        0 non-null       float64\n 35  trans_dest       0 non-null       float64\n 36  time_stamp       379616 non-null  object \n 37  Unnamed: 0.1     134347 non-null  float64\n 38  Unnamed: 0       22091 non-null   float64\n 39  clean_tweet      71714 non-null   object \n 40  tokenized_tweet  71875 non-null   object \ndtypes: float64(19), object(22)\nmemory usage: 118.7+ MB\n","output_type":"stream"}]}],"metadata":{"language_info":{"name":""},"kernelspec":{"display_name":"","name":""}},"nbformat":4,"nbformat_minor":5}